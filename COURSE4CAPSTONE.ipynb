{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone\n",
    "\n",
    "Last week's notebook discussed a cummulative project that would be used as a test of knowledge from this series of courses.\n",
    "This notebook will serve as a reference point for you while you work on said project. Included in this notebook is a set of answers to your tasks, based on a set dataset. Make sure in your final submission you are using a different dataset! \n",
    "\n",
    "You will be working on 4 tasks:\n",
    "1. __Data Processing__ \n",
    "2. __Classification__ \n",
    "3. __Regression__ \n",
    "4. __Recommender Sytstems__\n",
    "\n",
    "These tasks are each representative of one of the courses in the series. So if you need help with any one of these tasks, be sure to look back at those courses for reference! Along with the previous courses, there will be checkpoints with given solutions so you can check to make sure you are headed in the right direction. ___Good Luck!___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing\n",
    "\n",
    "## The Data\n",
    "\n",
    "For this final project you will be doing your work on a dataset of your choice. For reference, an example with checkpoint answers will be included. This example will be an amazon dataset, which does not need any cleaning before proper analysis. This dataset in particular can be found [here.](https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz)\n",
    "This dataset is a set of Home Improvement Product reviews on amazon. It is a rather large dataset, so our computation might take slightly longer than normal.\n",
    "\n",
    "### First Step: Imports\n",
    "\n",
    "In the next cell we will give you all of the imports you should need to do your project. Feel free to add more if you would like, but these should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy\n",
    "import scipy.optimize\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from nltk.stem.porter import PorterStemmer # Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Read the data and Fill your dataset\n",
    "\n",
    "Take care of int casting the votes and rating. Also __add this bit of code__ to your for loop, taking off the outer \" \":\n",
    "\n",
    "\" d['verified_purchase'] = d['verified_purchase'] == 'Y' \"\n",
    "\n",
    "This simple makes the verified purchase column be strictly true/false values rather than Y/N strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0          US     45610553   RMDCHWD0Y5OZ9  B00HH62VB6       618218723   \n",
      "1          US     14640079   RZSL0BALIYUNU  B003LRN53I       986692292   \n",
      "2          US      6111003   RIZR67JKUDBI0  B0006VMBHI       603261968   \n",
      "3          US      1546619  R27HL570VNL85F  B002B55TRG       575084461   \n",
      "4          US     12222213  R34EBU9QDWJ1GD  B00N1YPXW2       165236328   \n",
      "\n",
      "                                       product_title     product_category  \\\n",
      "0  AGPtekÂ® 10 Isolated Output 9V 12V 18V Guitar P...  Musical Instruments   \n",
      "1         Sennheiser HD203 Closed-Back DJ Headphones  Musical Instruments   \n",
      "2                   AudioQuest LP record clean brush  Musical Instruments   \n",
      "3      Hohner Inc. 560BX-BF Special Twenty Harmonica  Musical Instruments   \n",
      "4        Blue Yeti USB Microphone - Blackout Edition  Musical Instruments   \n",
      "\n",
      "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
      "0            3              0            1    N                 N   \n",
      "1            5              0            0    N                 Y   \n",
      "2            3              0            1    N                 Y   \n",
      "3            5              0            0    N                 Y   \n",
      "4            5              0            0    N                 Y   \n",
      "\n",
      "                                     review_headline  \\\n",
      "0                                        Three Stars   \n",
      "1                                         Five Stars   \n",
      "2                                        Three Stars   \n",
      "3  I purchase these for a friend in return for pl...   \n",
      "4                                         Five Stars   \n",
      "\n",
      "                                         review_body review_date  \n",
      "0        Works very good, but induces ALOT of noise.  2015-08-31  \n",
      "1             Nice headphones at a reasonable price.  2015-08-31  \n",
      "2                       removes dust. does not clean  2015-08-31  \n",
      "3  I purchase these for a friend in return for pl...  2015-08-31  \n",
      "4                            This is an awesome mic!  2015-08-31  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 904004 entries, 0 to 904003\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   marketplace        904004 non-null  object\n",
      " 1   customer_id        904004 non-null  int64 \n",
      " 2   review_id          904004 non-null  object\n",
      " 3   product_id         904004 non-null  object\n",
      " 4   product_parent     904004 non-null  int64 \n",
      " 5   product_title      904003 non-null  object\n",
      " 6   product_category   904004 non-null  object\n",
      " 7   star_rating        904004 non-null  int64 \n",
      " 8   helpful_votes      904004 non-null  int64 \n",
      " 9   total_votes        904004 non-null  int64 \n",
      " 10  vine               904004 non-null  object\n",
      " 11  verified_purchase  904004 non-null  object\n",
      " 12  review_headline    903998 non-null  object\n",
      " 13  review_body        903941 non-null  object\n",
      " 14  review_date        903996 non-null  object\n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 103.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '/Users/joeko/Js/NCKU/2024Fall/Coursera/Python/CAPSTONE/amazon_reviews_us_Musical_Instruments_v1_00.tsv'\n",
    "data = pd.read_csv(data_path, sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this setup properly, you __should__ shuffle your data (which you should do in your submission), but the checkpoint values would change so for the sake of this example we will ___not___ shuffle the data.\n",
    "\n",
    "### TODO 2: Split the data into a Training and Testing set\n",
    "\n",
    "Have Training be the first 80%, and testing be the remaining 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data entries: 904004\n",
      "Training set size: 723203\n",
      "Testing set size: 180801\n",
      "723203 180801\n",
      "Lengths should be: 2107824 526957\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle and split the dataset into training and testing sets\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f\"Total data entries: {len(data)}\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Testing set size: {len(test_set)}\")\n",
    "\n",
    "print(len(train_set), len(test_set))\n",
    "print(\"Lengths should be: 2107824 526957\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now delete your dataset\n",
    "You don't want any of your answers to come from your original dataset any longer, but rather your Training Set, this will help you to not make any mistakes later on, especialy when referencing the checkpoint solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "del data\n",
    "\n",
    "# Verify that the original dataset is deleted\n",
    "try:\n",
    "    print(data)\n",
    "except NameError:\n",
    "    print(\"Original dataset deleted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Extracting Basic Statistics\n",
    "\n",
    "Next you need to answer some questions through any means (i.e. write a function or just find the answer) all based on the __Training Set:__\n",
    "1. What is the __average rating__?\n",
    "2. What fraction of reviews are from __verified purchases__?\n",
    "3. How many __total users__ are there?\n",
    "4. How many __total items__ are there?\n",
    "5. What fraction of reviews have __5-star ratings__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rating: 4.2507497894781965\n",
      "Fraction of Verified Purchases: 0.8637837508970511\n",
      "Total Users: 481442\n",
      "Total Items: 111187\n",
      "Fraction of 5-Star Ratings: 0.6331749176925427\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# Ensure you have already loaded the train_set DataFrame\n",
    "\n",
    "# 1. Calculate the average rating\n",
    "average_rating = train_set['star_rating'].mean()\n",
    "print(f\"Average Rating: {average_rating}\")\n",
    "\n",
    "# 2. Calculate the fraction of reviews from verified purchases\n",
    "verified_fraction = (train_set['verified_purchase'] == 'Y').mean()\n",
    "print(f\"Fraction of Verified Purchases: {verified_fraction}\")\n",
    "\n",
    "# 3. Count the total number of unique users\n",
    "total_users = train_set['customer_id'].nunique()\n",
    "print(f\"Total Users: {total_users}\")\n",
    "\n",
    "# 4. Count the total number of unique items\n",
    "total_items = train_set['product_id'].nunique()\n",
    "print(f\"Total Items: {total_items}\")\n",
    "\n",
    "# 5. Calculate the fraction of reviews with a 5-star rating\n",
    "five_star_fraction = (train_set['star_rating'] == 5).mean()\n",
    "print(f\"Fraction of 5-Star Ratings: {five_star_fraction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "1. 4.219492709068689\n",
    "2. 0.9176558384381238\n",
    "3. 1396587\n",
    "4. 294787\n",
    "5. 0.642813631498645"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Classification\n",
    "\n",
    "Next you will use our knowledge of classification to extract features and make predictions based on them. Here you will be using a Logistic Regression Model, keep this in mind so you know where to get help from.\n",
    "\n",
    "### TODO 1: Define the feature function\n",
    "\n",
    "This implementation will be based on the __star rating__ and the ___length___ of the __review body__. Hint: Remember the offset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "# Define the feature function\n",
    "def feature_function(data):\n",
    "    \"\"\"\n",
    "    Extract features for logistic regression:\n",
    "    1. Offset (always 1)\n",
    "    2. Star rating\n",
    "    3. Review body length (word count)\n",
    "    \"\"\"\n",
    "    # Calculate the length of the review body\n",
    "    review_length = data['review_body'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Create the feature matrix\n",
    "    features = pd.DataFrame({\n",
    "        'offset': 1,  # Offset term\n",
    "        'star_rating': data['star_rating'],\n",
    "        'review_length': review_length\n",
    "    })\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        offset  star_rating  review_length\n",
      "235985       1            5             17\n",
      "398088       1            5             85\n",
      "893773       1            3             90\n",
      "813575       1            5            163\n",
      "288860       1            3              6\n"
     ]
    }
   ],
   "source": [
    "# Generate features for the training set\n",
    "X_train = feature_function(train_set)\n",
    "\n",
    "# Display the first few rows of the feature matrix\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Fit your model\n",
    "\n",
    "1. Create your __Feature Vector__ based on your feature function defined above. \n",
    "2. Create your __Label Vector__ based on the \"verified purchase\" column of your training set.\n",
    "3. Define your model as a __Logistic Regression__ model.\n",
    "4. Fit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete!\n",
      "Model coefficients: [[ 0.88858746  0.11158428 -0.00509123]]\n",
      "Model intercept: [0.88859591]\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Extract features for the training set\n",
    "X_train = feature_function(train_set)\n",
    "\n",
    "# Step 2: Create the label vector\n",
    "# Encode 'Y' as 1 and 'N' as 0 in the verified_purchase column\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_set['verified_purchase'])\n",
    "\n",
    "# Step 3: Initialize the Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Step 4: Train the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# Print the model coefficients\n",
    "print(\"Model coefficients:\", logistic_model.coef_)\n",
    "print(\"Model intercept:\", logistic_model.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Compute Accuracy of Your Model\n",
    "\n",
    "1. Make __Predictions__ based on your model.\n",
    "2. Compute the __Accuracy__ of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8638\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Extract features for the testing set\n",
    "X_test = feature_function(test_set)\n",
    "\n",
    "# Step 2: Encode the labels for the testing set\n",
    "y_test = label_encoder.transform(test_set['verified_purchase'])\n",
    "\n",
    "# Step 3: Generate predictions using the trained model\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Step 4: Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Finding the Balanced Error Rate\n",
    "\n",
    "1. Compute __True__ and __False Positives__\n",
    "2. Compute __True__ and __False Negatives__\n",
    "3. Compute __Balanced Error Rate__ based on your above defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Error Rate (BER): 0.4819\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Step 1: Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Step 2: Calculate FPR and FNR\n",
    "fpr = fp / (fp + tn)  # False Positive Rate\n",
    "fnr = fn / (fn + tp)  # False Negative Rate\n",
    "\n",
    "# Step 3: Calculate BER\n",
    "ber = (fpr + fnr) / 2\n",
    "print(f\"Balanced Error Rate (BER): {ber:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "3. Accuracy = 0.9172573231920692\n",
    "4. BER = 0.4895446422238072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Regression\n",
    "\n",
    "In this section you will start by working though two examples of altering features to further differentiate. Then you will work through how to evaluate a Regularaized model.\n",
    "\n",
    "Lets start by defining a new y vector, specific to our Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 3, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# Extract 'star_rating' column as a list\n",
    "y_reg = train_set['star_rating'].tolist()\n",
    "\n",
    "# Verify the result\n",
    "print(y_reg[:5])  # Print the first 5 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Unique Words in a Sample Set\n",
    "\n",
    "We are going to work with a smaller Sample Set here, as stemming on the normal training set will take a very long time. (Feel free to change sampleSet -> train_set if you would like to see)\n",
    "\n",
    "1. Count the number of unique words found within the 'review body' portion of the sample set defined below, making sure to __Ignore Punctuation and Capitalization__.\n",
    "2. Count the number of unique words found within the 'review body' portion of the sample set defined below, this time with use of __Stemming,__ __Ignoring Puctuation,__ ___and___ __Capitalization__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#GIVEN for 2.\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSet = train_set[:2*len(train_set)//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words (no stemming): 124564\n",
      "Unique words (with stemming): 103340\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Initialize stemmer and punctuation set\n",
    "stemmer = PorterStemmer()\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Sample the dataset (20% of training set for efficiency)\n",
    "sample_set = train_set.sample(frac=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Count unique words (ignoring punctuation and capitalization)\n",
    "word_count = defaultdict(int)\n",
    "for review in sample_set['review_body']:\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    cleaned_review = ''.join([c.lower() for c in str(review) if c not in punctuation])\n",
    "    # Count unique words\n",
    "    for word in cleaned_review.split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "# Step 2: Count unique words with stemming\n",
    "word_count_stem = defaultdict(int)\n",
    "for review in sample_set['review_body']:\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    cleaned_review = ''.join([c.lower() for c in str(review) if c not in punctuation])\n",
    "    # Apply stemming and count unique words\n",
    "    for word in cleaned_review.split():\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        word_count_stem[stemmed_word] += 1\n",
    "\n",
    "# Results\n",
    "print(f\"Unique words (no stemming): {len(word_count)}\")\n",
    "print(f\"Unique words (with stemming): {len(word_count_stem)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Evaluating Classifiers\n",
    "\n",
    "1. Given the feature function and your counts vector, __Define__ your X_reg vector. (This being the X vector, simply labeled for the Regression model)\n",
    "2. __Fit__ your model using a __Ridge Model__ with (alpha = 1.0, fit_intercept = True).\n",
    "3. Using your model, __Make your Predictions__.\n",
    "4. Find the __MSE__ between your predictions and your y_reg vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the feature_reg function\n",
    "def feature_reg(datum):\n",
    "    # Initialize a feature vector with zeros\n",
    "    feat = [0] * len(words)\n",
    "    # Ensure the review_body is a string, otherwise use an empty string\n",
    "    review_body = str(datum['review_body']) if isinstance(datum['review_body'], str) else \"\"\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    r = ''.join([c for c in review_body.lower() if c not in punctuation])\n",
    "    # Count word frequencies\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    return feat\n",
    "\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN COUNTS AND SETS\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#Note: increasing the size of the dictionary may require a lot of memory\n",
    "words = [x[1] for x in counts[:100]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words in the training set\n",
    "word_count = defaultdict(int)\n",
    "for review in train_set['review_body']:\n",
    "    review_text = str(review) if isinstance(review, str) else \"\"\n",
    "    cleaned_review = ''.join([c.lower() for c in review_text if c not in punctuation])\n",
    "    for word in cleaned_review.split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "# Create the top N words dictionary (e.g., top 100 words)\n",
    "N = 100  # Adjust the number as needed\n",
    "counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "words = [x[0] for x in counts[:N]]\n",
    "wordId = {word: i for i, word in enumerate(words)}\n",
    "wordSet = set(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.2245\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Create the feature matrix (X_reg) using the training set\n",
    "X_reg = np.array([feature_reg(row) for _, row in train_set.iterrows()])\n",
    "# Ridge Regression target variable\n",
    "y_reg = train_set['star_rating'].tolist()\n",
    "\n",
    "# Initialize and fit the Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0, fit_intercept=True)\n",
    "ridge_model.fit(X_reg, y_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ridge_model.predict(X_reg)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x - y) ** 2 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "mse = MSE(y_pred, y_reg)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: Unique words (no stemming): 343747\n",
      "Checkpoint: Unique words (with stemming): 100\n",
      "Checkpoint: Feature matrix shape: (723203, 100)\n",
      "Checkpoint: Mean Squared Error (MSE): 1.2245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize punctuation set\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# Step 1: Count words in the training set\n",
    "word_count = defaultdict(int)\n",
    "for review in train_set['review_body']:\n",
    "    review_text = str(review) if isinstance(review, str) else \"\"\n",
    "    cleaned_review = ''.join([c.lower() for c in review_text if c not in punctuation])\n",
    "    for word in cleaned_review.split():\n",
    "        word_count[word] += 1\n",
    "\n",
    "# Checkpoint: Unique word counts\n",
    "print(f\"Checkpoint: Unique words (no stemming): {len(word_count)}\")\n",
    "print(f\"Checkpoint: Unique words (with stemming): {len(words)}\")\n",
    "\n",
    "# Checkpoint: Feature matrix shape\n",
    "print(f\"Checkpoint: Feature matrix shape: {X_reg.shape}\")\n",
    "\n",
    "# Checkpoint: Mean Squared Error (MSE)\n",
    "print(f\"Checkpoint: Mean Squared Error (MSE): {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "1. len(wordCount) = 135769\n",
    "2. len(wordCountStem) = 113888\n",
    "4. MSE = 1.2869981011943792 (Roughly, could change slightly due to rounding errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to work with this example more in your free time, here are some tips to improve your solution:\n",
    "# 1. Implement a validation pipeline and tune the regularization parameter\n",
    "# 2. Alter the word features (e.g. dictionary size, punctuation, capitalization, stemming, etc.)\n",
    "# 3. Incorporate features other than word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Recommendation Systems\n",
    "\n",
    "For your final task, you will use your knowledge of simple latent factor-based recommender systems to make predictions. Then evaluating the performance of your predictions.\n",
    "\n",
    "### Starting up\n",
    "\n",
    "The next cell contains some starter code that you will need for your tasks in this section.\n",
    "Notice you are back to using the __train_set__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Calculate the ratingMean\n",
    "\n",
    "1. Find the __average rating__ of your training set.\n",
    "2. Calculate a __baseline MSE value__ from the actual ratings to the average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Mean: 4.2507\n",
      "Baseline Mean Squared Error (MSE): 1.4810\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# Step 1: Calculate the average rating\n",
    "rating_mean = train_set['star_rating'].mean()\n",
    "print(f\"Rating Mean: {rating_mean:.4f}\")\n",
    "\n",
    "# Step 2: Calculate the baseline MSE\n",
    "# Use the mean rating as a constant prediction for all items\n",
    "y_baseline = [rating_mean] * len(train_set)\n",
    "y_actual = train_set['star_rating'].tolist()\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x - y) ** 2 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "baseline_mse = MSE(y_baseline, y_actual)\n",
    "print(f\"Baseline Mean Squared Error (MSE): {baseline_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are defining the functions you will need to optimize your MSE value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "# Initialize dictionaries for user and item biases\n",
    "user_biases = defaultdict(float)\n",
    "item_biases = defaultdict(float)\n",
    "\n",
    "# Group reviews by user and item\n",
    "reviews_per_user = defaultdict(list)\n",
    "reviews_per_item = defaultdict(list)\n",
    "\n",
    "for _, row in train_set.iterrows():\n",
    "    user = row['customer_id']\n",
    "    item = row['product_id']\n",
    "    reviews_per_user[user].append(row)\n",
    "    reviews_per_item[item].append(row)\n",
    "\n",
    "# Number of users and items\n",
    "n_users = len(reviews_per_user)\n",
    "n_items = len(reviews_per_item)\n",
    "\n",
    "# User and item lists\n",
    "users = list(reviews_per_user.keys())\n",
    "items = list(reviews_per_item.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def prediction(user, item):\n",
    "    return alpha + user_biases[user] + item_biases[item]\n",
    "\n",
    "# Unpack the parameters\n",
    "def unpack(theta):\n",
    "    global alpha, user_biases, item_biases\n",
    "    alpha = theta[0]\n",
    "    user_biases = dict(zip(users, theta[1:n_users+1]))\n",
    "    item_biases = dict(zip(items, theta[1+n_users:]))\n",
    "\n",
    "# Cost function\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(row['customer_id'], row['product_id']) for _, row in train_set.iterrows()]\n",
    "    cost = MSE(predictions, labels)\n",
    "    # Add regularization terms\n",
    "    cost += lamb * sum(user_biases[u] ** 2 for u in user_biases)\n",
    "    cost += lamb * sum(item_biases[i] ** 2 for i in item_biases)\n",
    "    return cost\n",
    "\n",
    "# Derivative function\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    dalpha = 0\n",
    "    d_user_biases = defaultdict(float)\n",
    "    d_item_biases = defaultdict(float)\n",
    "\n",
    "    for _, row in train_set.iterrows():\n",
    "        user = row['customer_id']\n",
    "        item = row['product_id']\n",
    "        true_rating = row['star_rating']\n",
    "        pred = prediction(user, item)\n",
    "        diff = pred - true_rating\n",
    "        dalpha += 2 * diff / len(train_set)\n",
    "        d_user_biases[user] += 2 * diff / len(train_set)\n",
    "        d_item_biases[item] += 2 * diff / len(train_set)\n",
    "\n",
    "    for u in user_biases:\n",
    "        d_user_biases[u] += 2 * lamb * user_biases[u]\n",
    "    for i in item_biases:\n",
    "        d_item_biases[i] += 2 * lamb * item_biases[i]\n",
    "\n",
    "    grad = [dalpha] + [d_user_biases[u] for u in users] + [d_item_biases[i] for i in items]\n",
    "    return np.array(grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Optimize\n",
    "\n",
    "1. __Optimize__ your MSE using the scipy.optimize.fmin_1_bfgs_b(\"arguments\") functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Mean Squared Error (MSE): 1.4803\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "# Initial parameters\n",
    "alpha = rating_mean\n",
    "initial_user_biases = np.zeros(n_users)\n",
    "initial_item_biases = np.zeros(n_items)\n",
    "initial_theta = np.hstack(([alpha], initial_user_biases, initial_item_biases))\n",
    "\n",
    "# Labels (actual ratings)\n",
    "labels = train_set['star_rating'].tolist()\n",
    "\n",
    "# Regularization parameter\n",
    "lamb = 0.1\n",
    "\n",
    "# Optimize using scipy\n",
    "result = scipy.optimize.fmin_l_bfgs_b(cost, initial_theta, fprime=derivative, args=(labels, lamb))\n",
    "optimized_theta = result[0]\n",
    "\n",
    "# Unpack the optimized parameters\n",
    "unpack(optimized_theta)\n",
    "\n",
    "# Calculate the optimized MSE\n",
    "optimized_predictions = [prediction(row['customer_id'], row['product_id']) for _, row in train_set.iterrows()]\n",
    "optimized_mse = MSE(optimized_predictions, labels)\n",
    "print(f\"Optimized Mean Squared Error (MSE): {optimized_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Mean: 4.2507\n",
      "Baseline MSE: 1.4810\n",
      "Optimized Mean Squared Error (MSE): 1.4803\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import scipy.optimize\n",
    "\n",
    "# Step 1: Calculate the average rating (ratingMean)\n",
    "rating_mean = train_set['star_rating'].mean()\n",
    "print(f\"Rating Mean: {rating_mean:.4f}\")\n",
    "\n",
    "# Step 2: Calculate the baseline MSE (baseLine)\n",
    "y_baseline = [rating_mean] * len(train_set)\n",
    "y_actual = train_set['star_rating'].tolist()\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x - y) ** 2 for x, y in zip(predictions, labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "baseline_mse = MSE(y_baseline, y_actual)\n",
    "print(f\"Baseline MSE: {baseline_mse:.4f}\")\n",
    "\n",
    "print(f\"Optimized Mean Squared Error (MSE): {optimized_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint:\n",
    "\n",
    "Here is a list of answers for the questions above. Use these to reference how you are doing in finding the correct solutions.\n",
    "1. ratingMean = 4.219492709068689\n",
    "2. baseLine = 1.634495697493549\n",
    "3. optimized MSE -> converges to roughly 1.6083083....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're all done!\n",
    "\n",
    "Congratulations! This project was the end of 4 whole courses worth of content! This project clearly didn't cover every single topic from those courses, but it serves as a summary for everything you have learned. This is only the start of Python Data Projects, so continue to learn and good luck in your future endeavors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
